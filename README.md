# Customer-Behaviour-Insights
 Customer Behavior Insights Data Pipeline project.

It includes:

Project summary

Directory structure

Airflow & PySpark job overview

Setup instructions

Security & testing notes

Your author info with LinkedIn

Disclaimer to avoid legal issues



# ğŸ“¦ Customer Behavior Insights Data Pipeline (PySpark + AWS + Airflow)

This repository contains a simulated version of a real-world data engineering pipeline project that reflects the structure, logic, and tooling described in Mirza Ilyas Ali Baig's resume and project summaries.

## ğŸš€ Project Summary
A PySpark-based data pipeline that ingests data from:
- Amazon S3 (CSV/Parquet data)
- Snowflake (customer transaction tables)
- Web API (real-time location data)

The pipeline performs transformations and writes the final output to:
- Amazon S3 (as Parquet)
- Snowflake (analytics tables)

All jobs are scheduled and monitored using **Apache Airflow**.

---

## ğŸ“‚ Repository Structure
```bash
customer_behavior_insights/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ customer_behavior_dag.py        # Airflow DAG definition
â”‚
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ extract_s3_data.py              # PySpark job to read from S3
â”‚   â”œâ”€â”€ extract_snowflake_data.py       # PySpark job to read from Snowflake
â”‚   â”œâ”€â”€ extract_webapi_data.py          # PySpark job to read from Web API
â”‚   â”œâ”€â”€ master_aggregator.py            # Final job to combine and transform all sources
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ spark_session.py                # Spark session creation logic
â”‚   â”œâ”€â”€ secrets_manager.py              # AWS Secrets Manager utility
â”‚   â””â”€â”€ api_utils.py                    # API request logic
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline_config.yaml            # Configuration and source metadata
â”‚
â”œâ”€â”€ README.md                           # Project documentation
â””â”€â”€ requirements.txt                    # Python dependencies for local development
```

---

## âœ… Setup & Installation
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install required packages
pip install -r requirements.txt
```

---

## âš™ï¸ Airflow DAG
- Scheduled to run at 9:00 AM EST daily
- Includes steps:
  1. Start EMR cluster (simulated)
  2. Run extract_s3_data.py
  3. Run extract_snowflake_data.py
  4. Run extract_webapi_data.py
  5. Run master_aggregator.py
  6. Terminate EMR cluster

---

## ğŸ“Š Transformations
Performed in PySpark:
- Filters, joins, UDFs
- Aggregations and window functions
- Schema validation and error handling

---

## ğŸ›¡ï¸ Security
- Snowflake credentials stored in **AWS Secrets Manager**
- Retrieved securely in jobs via `boto3`

---

## ğŸ§ª Testing
- Unit tests (in progress)
- PySpark logic tested on local mode using `SparkSession(master=\"local\")`

---

## ğŸ‘¨â€ğŸ’» Author
**Mirza Ilyas Ali Baig**  
Data Engineer | AWS | PySpark | Airflow | Power BI  
[LinkedIn](https://www.linkedin.com/in/mirzailyas/)  

---

## ğŸ“œ Disclaimer
This is a sample/simulated project meant for showcasing skills and project understanding. It does not contain real client data or credentials.
